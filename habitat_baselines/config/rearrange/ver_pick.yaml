VERBOSE: False
BASE_TASK_CONFIG_PATH: configs/tasks/rearrange/pick.yaml
TRAINER_NAME: "ver"
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: ["disk"]
TENSORBOARD_DIR: "../ver_trains/pick_depth_only/tb"
VIDEO_DIR: "../ver_trains/pick_depth_only/video_dir"
VIDEO_FPS: 30
VIDEO_RENDER_TOP_DOWN: False
VIDEO_RENDER_ALL_INFO: True
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: "../ver_trains/pick_depth_only/checkpoints"
NUM_ENVIRONMENTS: 8
# Visual sensors to include
#SENSORS: ["HEAD_DEPTH_SENSOR", "HEAD_RGB_SENSOR"]
SENSORS: ["HEAD_DEPTH_SENSOR"]
CHECKPOINT_FOLDER: "../ver_trains/pick_depth_only/checkpoints"
NUM_UPDATES: -1
TOTAL_NUM_STEPS: 1.0e9
WRITER_TYPE: 'wb'
LOG_INTERVAL: 10
NUM_CHECKPOINTS: 20
# Force PyTorch to be single threaded as
# this improves performance considerably
FORCE_TORCH_SINGLE_THREADED: True
EVAL_KEYS_TO_INCLUDE_IN_NAME: ['reward', 'force', 'success']

WB:
  PROJECT_NAME: 'rearrangement_skill_training'
  RUN_NAME: 'depth_only'
  GROUP: 'pick_ver'

RL:
  POLICY:
      name: "PointNavResNetPolicy"
      action_distribution_type: "gaussian"
      ACTION_DIST:
         use_log_std: True
         clamp_std: True
         std_init: -1.0
         use_std_param: True
  PPO:
    # ppo params
    clip_param: 0.2
    ppo_epoch: 2
    num_mini_batch: 2
    value_loss_coef: 0.5
    entropy_coef: 0.0001
    lr: 3e-4
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 128
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 50

    use_normalized_advantage: False

    hidden_size: 512

    # Use double buffered sampling, typically helps
    # when environment time is similar or large than
    # policy inference time during rollout generation
    use_double_buffered_sampler: False


