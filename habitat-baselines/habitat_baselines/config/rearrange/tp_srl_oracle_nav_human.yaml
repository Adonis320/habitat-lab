# @package _global_

defaults:
  - /benchmark/rearrange: rearrange_easy_human
  - /habitat_baselines: habitat_baselines_rl_config_base
  - /habitat_baselines/rl/policy/obs_transforms:
    - add_virtual_keys_base
  - /habitat/task/actions:
    - rearrange_stop
    - human_nav_action
    - humanjoint_action
  - /habitat/simulator/sim_sensors@sim_sensors.third_rgb_sensor: third_rgb_sensor
  - override /habitat/simulator/agents@habitat.simulator.agents.main_agent: depth_head_human_vis
  - _self_

habitat:
  simulator:
    agents:
      main_agent:
        robot_urdf: '/Users/xavierpuig/Documents/human_sim_data/amass_male.urdf'
        ik_arm_urdf: '/Users/xavierpuig/Documents/human_sim_data/amass_male.urdf'
        robot_type: 'AmassHuman'
        amass_path: '/Users/xavierpuig/Documents/human_sim_data/amass_smpl_h/'
        body_model_path: '/Users/xavierpuig/Documents/human_sim_data/smplh/male/model.npz'
        human_type: "AmassHuman"
        sim_sensors:
          third_rgb_sensor:
            type: "HabitatSimRGBSensor"
            height: 212
            width: 212

habitat_baselines:
  verbose: False
  trainer_name: "ddppo"
  torch_gpu_id: 0
  tensorboard_dir: "tb"
  video_dir: "video_dir"
  video_fps: 30
  video_render_views:
    - "third_rgb_sensor"
  test_episode_count: 1
  eval_ckpt_path_dir: ""
  num_environments: 1
  writer_type: 'tb'
  checkpoint_folder: "data/new_checkpoints"
  num_updates: -1
  total_num_steps: 1.0e8
  log_interval: 10
  num_checkpoints: 20
  load_resume_state_config: False
  force_torch_single_threaded: True
  eval_keys_to_include_in_name: ['reward', 'force', 'composite_success']
  eval:
    video_option: ["disk"]
    use_ckpt_config: False
    should_load_ckpt: False

  rl:
    policy:
        name: "HierarchicalPolicy"
        obs_transforms:
          add_virtual_keys:
            virtual_keys:
              "nav_to_skill": 8
              "object_to_agent_gps_compass": 2
        hierarchical_policy:
          high_level_policy:
            add_arm_rest: False
            name: "FixedHighLevelPolicy"
          defined_skills:
            GT_NAV:
              skill_name: "OracleNavHumanPolicy"
              obs_skill_inputs: ["obj_start_sensor", "abs_obj_start_sensor", "obj_goal_sensor", "abs_obj_goal_sensor"]
              goal_sensors: ["obj_goal_sensor", "abs_obj_goal_sensor"]
              NAV_ACTION_NAME: "humanjoint_action"
              max_skill_steps: 300
              force_end_on_timeout: True
              stop_angle_thresh: 0.2
              stop_dist_thresh: 1.0

            human_pick:
              skill_name: "PlaceSkillPolicy"
              name: "PointNavResNetPolicy"
              obs_skill_inputs: ["obj_goal_sensor"]
              max_skill_steps: 200
              force_end_on_timeout: True
            human_place:
              skill_name: "PlaceSkillPolicy"
              name: "PointNavResNetPolicy"
              obs_skill_inputs: ["obj_goal_sensor"]
              max_skill_steps: 200
              force_end_on_timeout: True

            wait_skill:
              skill_name: "HumanWaitSkillPolicy"
              max_skill_steps: -1.0
              force_end_on_timeout: False

          use_skills:
            # Uncomment if you are also using these skills
            # open_cab: "NN_OPEN_CAB"
            # open_fridge: "NN_OPEN_FRIDGE"
            # close_cab: "NN_OPEN_CAB"
            # close_fridge: "NN_OPEN_FRIDGE"
            # pick: "human_pick"
            # place: "human_place"
            nav: "GT_NAV"
            nav_to_receptacle: "GT_NAV"
            wait: "wait_skill"
            # reset_arm: "reset_arm_skill"

    ppo:
      # ppo params
      clip_param: 0.2
      ppo_epoch: 2
      num_mini_batch: 2
      value_loss_coef: 0.5
      entropy_coef: 0.0001
      lr: 2.5e-4
      eps: 1e-5
      max_grad_norm: 0.2
      num_steps: 128
      use_gae: True
      gamma: 0.99
      tau: 0.95
      use_linear_clip_decay: False
      use_linear_lr_decay: False
      reward_window_size: 50

      use_normalized_advantage: False

      hidden_size: 512

      # Use double buffered sampling, typically helps
      # when environment time is similar or larger than
      # policy inference time during rollout generation
      use_double_buffered_sampler: False

    ddppo:
      sync_frac: 0.6
      # The PyTorch distributed backend to use
      distrib_backend: NCCL
      # Visual encoder backbone
      pretrained_weights: data/ddppo-models/gibson-2plus-resnet50.pth
      # Initialize with pretrained weights
      pretrained: False
      # Initialize just the visual encoder backbone with pretrained weights
      pretrained_encoder: False
      # Whether the visual encoder backbone will be trained.
      train_encoder: True
      # Whether to reset the critic linear layer
      reset_critic: False

      # Model parameters
      backbone: resnet18
      rnn_type: LSTM
      num_recurrent_layers: 2
