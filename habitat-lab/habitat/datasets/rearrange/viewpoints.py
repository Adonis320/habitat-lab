import itertools
import os
from collections import Counter
from enum import Enum, auto
from typing import List

import cv2
import imageio
import magnum as mn
import numpy as np

import habitat_sim
from habitat.core.simulator import AgentState
from habitat.datasets.rearrange.geometry_utils import direction_to_quaternion
from habitat.datasets.rearrange.viz_utils import (
    COLOR_PALETTE,
    draw_obj_bbox_on_topdown_map,
    get_topdown_map,
)
from habitat.tasks.nav.object_nav_task import ObjectViewLocation
from habitat.tasks.rearrange.utils import get_aabb
from habitat.tasks.utils import compute_pixel_coverage
from habitat_sim.utils.common import quat_to_coeffs

ISLAND_RADIUS_LIMIT = 3.5


class ViewpointType(Enum):
    too_far = auto()
    down_unnavigable = auto()
    island_too_small = auto()
    outdoor_viewpoint = auto()
    low_visibility = auto()
    good = auto()


def populate_semantic_graph(sim):
    rom = sim.get_rigid_object_manager()
    for handle in rom.get_object_handles():
        obj = rom.get_object_by_handle(handle)
        for node in obj.visual_scene_nodes:
            node.semantic_id = obj.object_id + 1


def generate_viewpoints(
    sim: habitat_sim.Simulator,
    obj,
    object_transform: mn.Matrix4 = None,
    debug_viz: bool = False,
) -> List[ObjectViewLocation]:
    """Generates a list of viewpoints for an object.
    A viewpoint is a 3D position and rotation from where the agent
    can see the object. The viewpoints are generated by sampling
    points on a grid around the object and then checking if the
    agent can see the object from each point using its semantic
    sensor."""
    assert obj is not None

    cached_obj_transform = obj.transformation
    if object_transform:
        obj.transformation = object_transform

    object_id = obj.object_id
    object_aabb = get_aabb(object_id, sim, transformed=True)
    object_position = object_aabb.center()

    center = np.array(obj.translation)
    sizes = np.array(obj.root_scene_node.cumulative_bb.size())
    rotation = obj.rotation
    object_obb = habitat_sim.geo.OBB(center, sizes, rotation)

    eps = 1e-5

    object_nodes = obj.visual_scene_nodes
    assert all(
        node.semantic_id == object_id + 1 for node in object_nodes
    ), "Semantic IDs are not the right values. Did you populate the semantic graph?"
    semantic_id = object_nodes[0].semantic_id

    object_nodes = obj.visual_scene_nodes
    assert all(
        node.semantic_id == object_id + 1 for node in object_nodes
    ), "Semantic IDs are not the right values. Did you populate the semantic graph?"
    semantic_id = object_nodes[0].semantic_id

    max_distance = 1.0
    cell_size = 0.3 / 2.0
    x_len, _, z_len = object_aabb.size() / 2.0 + mn.Vector3(max_distance)
    x_bxp = np.arange(-x_len, x_len + eps, step=cell_size) + object_position[0]
    z_bxp = np.arange(-z_len, z_len + eps, step=cell_size) + object_position[2]
    candidate_poses = [
        np.array([x, object_position[1], z])
        for x, z in itertools.product(x_bxp, z_bxp)
    ]

    def down_is_navigable(pt, search_dist=2.0):
        pf = sim.pathfinder
        delta_y = 0.05
        max_steps = int(search_dist / delta_y)
        step = 0
        is_navigable = pf.is_navigable(pt, 2)
        while not is_navigable:
            pt[1] -= delta_y
            is_navigable = pf.is_navigable(pt)
            step += 1
            if step == max_steps:
                return False
        return True

    def _get_iou(x, y, z):
        pt = np.array([x, y, z])

        if not object_obb.distance(pt) <= max_distance:
            return -1, pt, None, ViewpointType.too_far

        if not down_is_navigable(pt):
            return -1, pt, None, ViewpointType.down_unnavigable

        pf = sim.pathfinder
        pt = np.array(pf.snap_point(pt))
        if pf.island_radius(pt) < ISLAND_RADIUS_LIMIT:
            return -1, pt, None, ViewpointType.island_too_small
        pt[1] += pf.nav_mesh_settings.agent_height

        goal_direction = object_position - pt
        goal_direction[1] = 0

        q = direction_to_quaternion(goal_direction)

        agent = sim.get_agent(0)
        agent_state = agent.get_state()
        agent_state.position = pt
        agent_state.rotation = q
        agent.set_state(agent_state)

        cov = 0
        for act_idx, act in enumerate(
            [
                "look_down",
                "look_up",
                "look_up",
            ]
        ):
            agent.act(act)
            obs = sim.get_sensor_observations(0)
            cov += compute_pixel_coverage(obs["semantic"], semantic_id)

            if debug_viz:
                rgb_obs = np.ascontiguousarray(obs["color"][..., :3])
                sem_obs = (obs["semantic"] == semantic_id).astype(
                    np.uint8
                ) * 255
                contours, _ = cv2.findContours(
                    sem_obs, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE
                )
                rgb_obs = cv2.drawContours(
                    rgb_obs, contours, -1, (0, 255, 0), 4
                )
                img_dir = "data/images/objnav_dataset_gen"
                os.makedirs(img_dir, exist_ok=True)
                imageio.imsave(
                    os.path.join(
                        img_dir,
                        f"{obj.handle}_{semantic_id}_{x}_{z}_{act_idx}.png",
                    ),
                    rgb_obs,
                )

        pt[1] -= pf.nav_mesh_settings.agent_height

        keep_thresh = 0.001
        if cov < keep_thresh:
            return -1, pt, None, ViewpointType.low_visibility

        return cov, pt, q, ViewpointType.good

    candidate_poses_ious_orig = [_get_iou(*pos) for pos in candidate_poses]
    poses_type_counter: Counter = Counter()
    for p in candidate_poses_ious_orig:
        poses_type_counter[p[-1]] += 1
    candidate_poses_ious = [p for p in candidate_poses_ious_orig if p[0] > 0]

    view_locations = [
        ObjectViewLocation(
            AgentState(pt.tolist(), quat_to_coeffs(q).tolist()), iou
        )
        for iou, pt, q, _ in candidate_poses_ious
    ]
    view_locations = sorted(view_locations, reverse=True, key=lambda v: v.iou)

    if debug_viz and len(view_locations) == 0:
        object_position_on_floor = np.array(object_position).copy()
        if len(view_locations) > 0:
            object_position_on_floor[1] = view_locations[
                0
            ].agent_state.position[1]
        else:
            while True:
                # TODO: think of better way than ->
                pf = sim.pathfinder
                navigable_point = pf.get_random_navigable_point()
                if pf.island_radius(navigable_point) >= ISLAND_RADIUS_LIMIT:
                    break

            object_position_on_floor[1] = navigable_point[1]

        topdown_map = get_topdown_map(
            sim, start_pos=object_position_on_floor, marker=None
        )

        colors = list(COLOR_PALETTE.values())
        for p in candidate_poses_ious_orig:
            if p[0] < 0:
                color = colors[p[-1].value - 1]
            else:
                color = COLOR_PALETTE["green"]

            topdown_map = get_topdown_map(
                sim,
                start_pos=p[1],
                topdown_map=topdown_map,
                marker="circle",
                radius=2,
                color=color,
            )

        topdown_map = get_topdown_map(
            sim,
            start_pos=object_position_on_floor,
            topdown_map=topdown_map,
            marker="circle",
            radius=6,
            color=COLOR_PALETTE["red"],
        )
        topdown_map = draw_obj_bbox_on_topdown_map(
            topdown_map, object_aabb, sim
        )

        if len(view_locations) == 0:
            h = topdown_map.shape[0]
            topdown_map = cv2.copyMakeBorder(
                topdown_map,
                0,
                150,
                0,
                0,
                cv2.BORDER_CONSTANT,
                value=COLOR_PALETTE["white"],
            )

            for i, c in enumerate(poses_type_counter.items()):
                line = f"{c[0].name}: {c[1]}/{len(candidate_poses_ious_orig)}"
                color = colors[c[0].value - 1]
                topdown_map = cv2.putText(
                    topdown_map,
                    line,
                    (10, h + 25 * i + 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    color,
                    1,
                    cv2.LINE_AA,
                )

        img_dir = "data/images/objnav_dataset_gen/maps"
        os.makedirs(img_dir, exist_ok=True)
        imageio.imsave(
            os.path.join(img_dir, f"{obj.handle}_{semantic_id}.png"),
            topdown_map,
        )

    obj.transformation = cached_obj_transform

    return view_locations
